"""
Schema Embedder - BigQuery ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ChromaDBì— ì„ë² ë”©
"""

import os
import json
import hashlib
from datetime import datetime
from typing import Dict, List, Optional
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.schema import Document
from core.config import LLM_CONFIG

class SchemaEmbedder:
    def __init__(self, persist_directory: str = "./chroma_db"):
        """
        ìŠ¤í‚¤ë§ˆ ì„ë² ë”© ì´ˆê¸°í™”
        
        Args:
            persist_directory: ChromaDB ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.persist_directory = persist_directory
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = None
        self.collection_name = "bigquery_schemas"
        self.cache_metadata_file = os.path.join(persist_directory, "schema_cache.json")
        
    def initialize_vectorstore(self):
        """ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        try:
            # persist_directory ìƒì„±
            os.makedirs(self.persist_directory, exist_ok=True)
            
            self.vectorstore = Chroma(
                collection_name=self.collection_name,
                embedding_function=self.embeddings,
                persist_directory=self.persist_directory
            )
            print(f"âœ… ChromaDB ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ: {self.persist_directory}")
            return True
        except Exception as e:
            print(f"âŒ ChromaDB ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False
    
    def create_table_documents(self, schema_info: Dict) -> List[Document]:
        """
        ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ Document í˜•íƒœë¡œ ë³€í™˜
        
        Args:
            schema_info: BigQuery ìŠ¤í‚¤ë§ˆ ì •ë³´
            
        Returns:
            Document ë¦¬ìŠ¤íŠ¸
        """
        documents = []
        
        for table_name, schema in schema_info.items():
            # í…Œì´ë¸” ê¸°ë³¸ ì •ë³´ ë¬¸ì„œ
            table_doc_content = f"""í…Œì´ë¸”: {table_name}
ì„¤ëª…: {schema.get('description', 'ì„¤ëª… ì—†ìŒ')}
ì»¬ëŸ¼ ìˆ˜: {len(schema.get('columns', []))}

ì»¬ëŸ¼ ì •ë³´:"""
            
            # ì»¬ëŸ¼ ì •ë³´ ì¶”ê°€
            for col in schema.get('columns', []):
                col_desc = col.get('description', '')
                table_doc_content += f"""
- {col['name']} ({col['type']}, {col['mode']})"""
                if col_desc:
                    table_doc_content += f": {col_desc}"
            
            # í…Œì´ë¸” ë¬¸ì„œ ìƒì„±
            table_document = Document(
                page_content=table_doc_content,
                metadata={
                    "type": "table",
                    "table_name": table_name,
                    "dataset": table_name.split('.')[0] if '.' in table_name else '',
                    "table_id": table_name.split('.')[1] if '.' in table_name else table_name,
                    "column_count": len(schema.get('columns', [])),
                    "description": schema.get('description', '')
                }
            )
            documents.append(table_document)
            
            # ê° ì»¬ëŸ¼ì„ ê°œë³„ ë¬¸ì„œë¡œë„ ìƒì„± (ë” ì„¸ë°€í•œ ê²€ìƒ‰ì„ ìœ„í•´)
            for col in schema.get('columns', []):
                col_content = f"""í…Œì´ë¸”: {table_name}
ì»¬ëŸ¼: {col['name']}
íƒ€ì…: {col['type']}
ëª¨ë“œ: {col['mode']}
ì„¤ëª…: {col.get('description', 'ì„¤ëª… ì—†ìŒ')}

ì´ ì»¬ëŸ¼ì€ {table_name} í…Œì´ë¸”ì˜ {col['type']} íƒ€ì… í•„ë“œì…ë‹ˆë‹¤."""
                
                col_document = Document(
                    page_content=col_content,
                    metadata={
                        "type": "column",
                        "table_name": table_name,
                        "column_name": col['name'],
                        "column_type": col['type'],
                        "column_mode": col['mode'],
                        "column_description": col.get('description', ''),
                        "dataset": table_name.split('.')[0] if '.' in table_name else '',
                        "table_id": table_name.split('.')[1] if '.' in table_name else table_name
                    }
                )
                documents.append(col_document)
        
        return documents
    
    def generate_config_hash(self) -> str:
        """BigQuery ì„¤ì • ì •ë³´ì˜ í•´ì‹œê°’ ìƒì„±"""
        from core.config import BIGQUERY_CONFIG
        
        # BigQuery ì„¤ì • ì •ë³´ë¥¼ í•´ì‹œì— í¬í•¨
        config_data = {
            "keyfile_path": BIGQUERY_CONFIG.get("keyfile_path", ""),
            "default_dataset": BIGQUERY_CONFIG.get("default_dataset", ""),
            "target_tables": BIGQUERY_CONFIG.get("target_tables", [])
        }
        
        # í‚¤íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ë‚´ìš©ë„ í•´ì‹œì— í¬í•¨ (project_id ë³€ê²½ ê°ì§€)
        keyfile_path = config_data["keyfile_path"]
        if os.path.exists(keyfile_path):
            try:
                with open(keyfile_path, 'r') as f:
                    keyfile_data = json.load(f)
                    config_data["project_id"] = keyfile_data.get("project_id", "")
            except Exception:
                pass
        
        config_str = json.dumps(config_data, sort_keys=True, ensure_ascii=False)
        return hashlib.sha256(config_str.encode('utf-8')).hexdigest()
    
    def generate_schema_hash(self, schema_info: Dict) -> str:
        """ìŠ¤í‚¤ë§ˆ ì •ë³´ì˜ í•´ì‹œê°’ ìƒì„±"""
        # ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ì •ë ¬ëœ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        schema_str = json.dumps(schema_info, sort_keys=True, ensure_ascii=False)
        # SHA256 í•´ì‹œ ìƒì„±
        return hashlib.sha256(schema_str.encode('utf-8')).hexdigest()
    
    def load_cache_metadata(self) -> Dict:
        """ìºì‹œ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        try:
            if os.path.exists(self.cache_metadata_file):
                with open(self.cache_metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}
    
    def save_cache_metadata(self, schema_hash: str, schema_info: Dict):
        """ìºì‹œ ë©”íƒ€ë°ì´í„°ì™€ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì €ì¥"""
        try:
            config_hash = self.generate_config_hash()
            metadata = {
                "schema_hash": schema_hash,
                "config_hash": config_hash,  # BigQuery ì„¤ì • í•´ì‹œ ì¶”ê°€
                "last_updated": datetime.now().isoformat(),
                "table_count": len(schema_info),
                "table_names": list(schema_info.keys()),
                "schema_data": schema_info  # ìŠ¤í‚¤ë§ˆ ì •ë³´ë„ í•¨ê»˜ ì €ì¥
            }
            
            with open(self.cache_metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
                
            print(f"ğŸ’¾ ìºì‹œ ë©”íƒ€ë°ì´í„° ë° ìŠ¤í‚¤ë§ˆ ì •ë³´ ì €ì¥ ì™„ë£Œ: {self.cache_metadata_file}")
            
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def is_cache_valid(self, schema_info: Dict) -> bool:
        """ìºì‹œê°€ ìœ íš¨í•œì§€ í™•ì¸"""
        current_hash = self.generate_schema_hash(schema_info)
        cached_metadata = self.load_cache_metadata()
        
        if not cached_metadata:
            print("ğŸ“‹ ìºì‹œ ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        cached_hash = cached_metadata.get("schema_hash")
        if not cached_hash:
            print("ğŸ“‹ ì €ì¥ëœ ìŠ¤í‚¤ë§ˆ í•´ì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ë²¡í„°ìŠ¤í† ì–´ì— ì‹¤ì œ ë¬¸ì„œê°€ ìˆëŠ”ì§€ë„ í™•ì¸
        try:
            if self.vectorstore:
                collection = self.vectorstore._collection
                doc_count = collection.count()
                if doc_count == 0:
                    print("ğŸ“‹ ë²¡í„°ìŠ¤í† ì–´ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    return False
        except Exception as e:
            print(f"ğŸ“‹ ë²¡í„°ìŠ¤í† ì–´ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
        
        if current_hash == cached_hash:
            last_updated = cached_metadata.get("last_updated", "ì•Œ ìˆ˜ ì—†ìŒ")
            table_count = cached_metadata.get("table_count", 0)
            print(f"âœ… ìºì‹œê°€ ìœ íš¨í•©ë‹ˆë‹¤!")
            print(f"   - ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {last_updated}")
            print(f"   - í…Œì´ë¸” ìˆ˜: {table_count}")
            print(f"   - ë²¡í„° ë¬¸ì„œ ìˆ˜: {doc_count}")
            return True
        else:
            print("ğŸ”„ ìŠ¤í‚¤ë§ˆê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ì„ë² ë”©í•©ë‹ˆë‹¤.")
            print(f"   - ì´ì „ í•´ì‹œ: {cached_hash[:12]}...")
            print(f"   - í˜„ì¬ í•´ì‹œ: {current_hash[:12]}...")
            return False
    
    def get_cached_schema_info(self) -> Dict:
        """ìºì‹œëœ ìŠ¤í‚¤ë§ˆ ì •ë³´ ë°˜í™˜"""
        cached_metadata = self.load_cache_metadata()
        if cached_metadata and "schema_data" in cached_metadata:
            return cached_metadata["schema_data"]
        return {}
    
    def has_valid_cache(self) -> bool:
        """ìºì‹œ ìœ íš¨ì„± í™•ì¸ (BigQuery ì„¤ì • ê¸°ë°˜)"""
        try:
            cached_metadata = self.load_cache_metadata()
            if not cached_metadata or not cached_metadata.get("config_hash"):
                return False
            
            # BigQuery ì„¤ì •ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
            current_config_hash = self.generate_config_hash()
            cached_config_hash = cached_metadata.get("config_hash")
            
            if current_config_hash != cached_config_hash:
                print("ğŸ”„ BigQuery ì„¤ì •ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.")
                return False
            
            # ë²¡í„°ìŠ¤í† ì–´ì— ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸
            if self.vectorstore:
                collection = self.vectorstore._collection
                doc_count = collection.count()
                if doc_count > 0:
                    print("âœ… BigQuery ì„¤ì •ì´ ë™ì¼í•˜ê³  ìºì‹œê°€ ìœ íš¨í•©ë‹ˆë‹¤!")
                    return True
            
            return False
        except Exception as e:
            print(f"ğŸ“‹ ìºì‹œ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def embed_schemas(self, schema_info: Dict) -> bool:
        """
        ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ë²¡í„°ìŠ¤í† ì–´ì— ì„ë² ë”© (ìºì‹± ì§€ì›)
        
        Args:
            schema_info: BigQuery ìŠ¤í‚¤ë§ˆ ì •ë³´
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not self.vectorstore:
            print("âŒ ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            # ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬
            if self.is_cache_valid(schema_info):
                print("ğŸ¯ ê¸°ì¡´ ì„ë² ë”© ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return True
            
            # ìºì‹œê°€ ë¬´íš¨í•œ ê²½ìš°ì—ë§Œ ìƒˆë¡œ ì„ë² ë”©
            print("ğŸ” ìŠ¤í‚¤ë§ˆ ë¬¸ì„œ ìƒì„± ì¤‘...")
            documents = self.create_table_documents(schema_info)
            print(f"ğŸ“ ìƒì„±ëœ ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ")
            
            # ìŠ¤í‚¤ë§ˆê°€ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ ê¸°ì¡´ ë°ì´í„° êµì²´
            print("ğŸ”„ ê¸°ì¡´ ì„ë² ë”© ë°ì´í„° êµì²´ ì¤‘...")
            try:
                collection = self.vectorstore._collection
                existing_count = collection.count()
                
                if existing_count > 0:
                    # ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ
                    all_ids = collection.get()['ids']
                    if all_ids:
                        collection.delete(ids=all_ids)
                        print(f"   - {len(all_ids)}ê°œ ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œë¨")
                else:
                    print("   - ê¸°ì¡´ ë¬¸ì„œ ì—†ìŒ (ì²« ì„ë² ë”©)")
                    
            except Exception as e:
                print(f"   - ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
            
            # ìƒˆ ë¬¸ì„œ ì„ë² ë”© ë° ì €ì¥
            print("âš¡ ìƒˆ ìŠ¤í‚¤ë§ˆ ì„ë² ë”© ì§„í–‰ ì¤‘...")
            self.vectorstore.add_documents(documents)
            
            # ChromaDBëŠ” ìë™ìœ¼ë¡œ persistë˜ë¯€ë¡œ ë³„ë„ persist() í˜¸ì¶œ ë¶ˆí•„ìš”
            print("ğŸ’¾ ë²¡í„°ìŠ¤í† ì–´ ìë™ ì €ì¥ë¨")
            
            # ìºì‹œ ë©”íƒ€ë°ì´í„° ì €ì¥
            schema_hash = self.generate_schema_hash(schema_info)
            self.save_cache_metadata(schema_hash, schema_info)
            
            print(f"âœ… ìŠ¤í‚¤ë§ˆ ì„ë² ë”© ì™„ë£Œ!")
            print(f"   - í…Œì´ë¸” ìˆ˜: {len(schema_info)}ê°œ")
            print(f"   - ì´ ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ")
            print(f"   - ì €ì¥ ìœ„ì¹˜: {self.persist_directory}")
            print(f"   - ìŠ¤í‚¤ë§ˆ í•´ì‹œ: {schema_hash[:12]}...")
            
            return True
            
        except Exception as e:
            print(f"âŒ ìŠ¤í‚¤ë§ˆ ì„ë² ë”© ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def get_collection_info(self) -> Dict:
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ (ìºì‹œ ì •ë³´ í¬í•¨)"""
        if not self.vectorstore:
            return {}
        
        try:
            collection = self.vectorstore._collection
            count = collection.count()
            
            info = {
                "collection_name": self.collection_name,
                "document_count": count,
                "persist_directory": self.persist_directory
            }
            
            # ìºì‹œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            cache_metadata = self.load_cache_metadata()
            if cache_metadata:
                info.update({
                    "cache_last_updated": cache_metadata.get("last_updated"),
                    "cache_table_count": cache_metadata.get("table_count"),
                    "cache_schema_hash": cache_metadata.get("schema_hash", "")[:12] + "..."
                })
            
            return info
            
        except Exception as e:
            print(f"âŒ ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def clear_cache(self):
        """ìºì‹œ ì™„ì „ ì‚­ì œ"""
        try:
            print("ğŸ—‘ï¸ ìºì‹œ ì‚­ì œ ì¤‘...")
            
            # ìºì‹œ ë©”íƒ€ë°ì´í„° íŒŒì¼ ì‚­ì œ
            if os.path.exists(self.cache_metadata_file):
                os.remove(self.cache_metadata_file)
                print("   - ìºì‹œ ë©”íƒ€ë°ì´í„° ì‚­ì œë¨")
            
            # ChromaDB ë²¡í„°ìŠ¤í† ì–´ ì‚­ì œ
            if self.vectorstore:
                try:
                    collection = self.vectorstore._collection
                    if collection.count() > 0:
                        all_ids = collection.get()['ids']
                        if all_ids:
                            collection.delete(ids=all_ids)
                            print(f"   - {len(all_ids)}ê°œ ë²¡í„° ë¬¸ì„œ ì‚­ì œë¨")
                except Exception as e:
                    print(f"   - ë²¡í„°ìŠ¤í† ì–´ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
            
            print("âœ… ìºì‹œ ì‚­ì œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
    
    def initialize_with_cache(self, bq_client) -> Dict:
        """
        ìºì‹œë¥¼ í™œìš©í•œ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”
        ìºì‹œê°€ ìœ íš¨í•˜ë©´ BigQuery API í˜¸ì¶œ ì—†ì´ ìºì‹œëœ ë°ì´í„° ì‚¬ìš©
        """
        print("ğŸ” ìºì‹œ ê¸°ë°˜ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì¤‘...")
        
        # ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
        if not self.initialize_vectorstore():
            return {}
        
        # ìºì‹œ ìœ íš¨ì„± í™•ì¸
        if self.has_valid_cache():
            print("âœ… ìœ íš¨í•œ ìºì‹œ ë°œê²¬!")
            cached_schema = self.get_cached_schema_info()
            if cached_schema:
                print(f"ğŸ¯ ìºì‹œëœ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©: {len(cached_schema)}ê°œ í…Œì´ë¸” (BigQuery API í˜¸ì¶œ ì—†ìŒ)")
                cached_metadata = self.load_cache_metadata()
                if cached_metadata:
                    last_updated = cached_metadata.get("last_updated", "").split('T')[0]
                    print(f"ğŸ“… ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {last_updated}")
                # BigQuery í´ë¼ì´ì–¸íŠ¸ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê³  ìºì‹œëœ ë°ì´í„°ë§Œ ë°˜í™˜
                return cached_schema
        
        # ìºì‹œê°€ ì—†ê±°ë‚˜ ë¬´íš¨í•œ ê²½ìš° BigQueryì—ì„œ ìƒˆë¡œ ì¡°íšŒ
        print("ğŸ”— BigQueryì—ì„œ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ ì¤‘...")
        if not bq_client.connect():
            return {}
        
        schema_info = bq_client.initialize_schema()
        if not schema_info:
            return {}
        
        # ìƒˆë¡œ ì¡°íšŒí•œ ìŠ¤í‚¤ë§ˆ ì„ë² ë”©
        if not self.embed_schemas(schema_info):
            return {}
        
        return schema_info

# ì „ì—­ ì„ë² ë” ì¸ìŠ¤í„´ìŠ¤
schema_embedder = SchemaEmbedder()