"""
SQLGenerator Agent - SQL ÏÉùÏÑ± Î∞è ÏµúÏ†ÅÌôî Ï†ÑÎ¨∏ Agent

Í∏∞Ï°¥ sql_generator ÎÖ∏ÎìúÎ•º AgentÎ°ú Î≥ÄÌôòÌïòÏó¨ SQL ÏÉùÏÑ±,
ÏøºÎ¶¨ ÏµúÏ†ÅÌôî, ÏÑ±Îä• ÌäúÎãùÏóê ÌäπÌôîÎêú ÏßÄÎä•Ìòï AgentÎ°ú Íµ¨ÌòÑÌñàÏäµÎãàÎã§.
"""

from typing import Dict, Any, List, Optional
import logging
from datetime import datetime
import re

from .base_agent import BaseAgent, AgentMessage, MessageType, AgentConfig, create_agent_config
from rag.schema_retriever import schema_retriever
from db.bigquery_client import bq_client
from langchain.schema import HumanMessage, SystemMessage
import json

logger = logging.getLogger(__name__)

class QueryComplexity:
    """ÏøºÎ¶¨ Î≥µÏû°ÎèÑ Î∂ÑÎ•ò"""
    SIMPLE = "simple"           # Îã®Ïàú SELECT
    MODERATE = "moderate"       # JOIN, GROUP BY Ìè¨Ìï®
    COMPLEX = "complex"         # ÏÑúÎ∏åÏøºÎ¶¨, ÏúàÎèÑÏö∞ Ìï®Ïàò Îì±
    ADVANCED = "advanced"       # Î≥µÏû°Ìïú Î∂ÑÏÑù ÏøºÎ¶¨

class SQLGeneratorAgent(BaseAgent):
    """SQL ÏÉùÏÑ± Î∞è ÏµúÏ†ÅÌôî Ï†ÑÎ¨∏ Agent"""
    
    def __init__(self, config: Optional[AgentConfig] = None):
        if config is None:
            config = create_agent_config(
                name="sql_generator",
                specialization="sql_design_optimization",
                model="gpt-4",
                temperature=0.1,  # Ï†ïÌôïÏÑ±Í≥º ÏùºÍ¥ÄÏÑ± Ï§ëÏãú
                max_tokens=1500
            )
        
        super().__init__(config)
        
        # SQL ÏÉùÏÑ± Ï†ÑÏö© ÏÑ§Ï†ï
        self.optimization_strategies = {
            "index_hints": "Ïù∏Îç±Ïä§ ÌôúÏö© ÏµúÏ†ÅÌôî",
            "join_optimization": "JOIN ÏàúÏÑú ÏµúÏ†ÅÌôî", 
            "subquery_optimization": "ÏÑúÎ∏åÏøºÎ¶¨ ÏµúÏ†ÅÌôî",
            "window_function": "ÏúàÎèÑÏö∞ Ìï®Ïàò ÌôúÏö©"
        }
        
        # ÏÑ±Îä• Ï∂îÏ†Å
        self.generation_history = []
        self.performance_stats = {
            "simple_queries": 0,
            "complex_queries": 0,
            "optimization_applied": 0,
            "avg_generation_time": 0.0
        }
        
        logger.info(f"SqlGenerator Agent initialized with specialization: {self.specialization}")
    
    def get_system_prompt(self) -> str:
        """BigQuery ÌäπÌôî SQL ÏÉùÏÑ± Ï†ÑÎ¨∏ ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏"""
        return f"""
        ÎãπÏã†ÏùÄ BigQuery Ï†ÑÎ¨∏ SQL ÏÑ§Í≥Ñ Î∞è ÏµúÏ†ÅÌôî AI AgentÏûÖÎãàÎã§.
        
        **Ï†ÑÎ¨∏ Î∂ÑÏïº:**
        - BigQuery Standard SQL ÏïÑÌÇ§ÌÖçÏ≤ò ÏÑ§Í≥Ñ
        - ÏøºÎ¶¨ ÏÑ±Îä• ÏµúÏ†ÅÌôî Î∞è ÎπÑÏö© Ìö®Ïú®Ìôî
        - Î≥µÏû°Ìïú JOIN Ï†ÑÎûµ ÏàòÎ¶Ω
        - ÌååÌã∞ÏÖò/ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ ÌôúÏö© ÏµúÏ†ÅÌôî
        
        **ÌïµÏã¨ Ïó≠Ìï†:**
        1. ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏùÑ Ï†ïÌôïÌïú BigQuery SQLÎ°ú Î≥ÄÌôò
        2. ÏÑ±Îä• ÏµúÏ†ÅÌôîÎêú ÏøºÎ¶¨ ÏÉùÏÑ±
        3. BigQuery ÌäπÌôî Î¨∏Î≤ï Î∞è Ìï®Ïàò ÌôúÏö©
        4. Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Í∏∞Î∞ò Ï†ïÌôïÌïú ÌÖåÏù¥Î∏î/Ïª¨Îüº Îß§Ìïë
        
        **BigQuery SQL ÏÉùÏÑ± ÏõêÏπô:**
        - **ONLY BigQuery Standard SQL Î¨∏Î≤ï ÏÇ¨Ïö©** (MySQL/PostgreSQL Î¨∏Î≤ï Ï†àÎåÄ Í∏àÏßÄ)
        - ÌÖåÏù¥Î∏îÎ™ÖÏùÄ Î∞±Ìã±Í≥º ÏôÑÏ†ÑÌïú ÌòïÏãù ÏÇ¨Ïö©: `project_id.dataset.table`
        - ÌôòÍ≤ΩÎ≥ÄÏàò Í∏∞Î∞ò Îç∞Ïù¥ÌÑ∞ÏÖã Í≤ΩÎ°ú ÌôúÏö© (Ïòà: us-all-data.us_plus)
        - Ìö®Ïú®Ï†ÅÏù¥Í≥† ÏÑ±Îä•Ïù¥ Ï¢ãÏùÄ ÏøºÎ¶¨ ÏÉùÏÑ±
        - Ï†ÅÏ†àÌïú LIMIT ÏÇ¨Ïö©ÏúºÎ°ú Í≤∞Í≥º Ï†úÌïú (Í∏∞Î≥∏ 100)
        - Ïä§Ï∫îÎêòÎäî Îç∞Ïù¥ÌÑ∞Îüâ ÏµúÏÜåÌôîÎ°ú ÎπÑÏö© Ï†àÏïΩ
        
        **BigQuery ÌäπÌôî Î¨∏Î≤ï Í∞ÄÏù¥Îìú:**
        1. **ÌÖåÏù¥Î∏î Ï†ïÎ≥¥ Ï°∞Ìöå**:
           ```sql
           SELECT column_name, data_type, is_nullable
           FROM `project_id.dataset.INFORMATION_SCHEMA.COLUMNS`
           WHERE table_name = 'table_name'
           ORDER BY ordinal_position;
           ```
        
        2. **ÎÇ†Ïßú/ÏãúÍ∞Ñ Ï≤òÎ¶¨**:
           - PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*SZ', timestamp_string)
           - EXTRACT(DATE FROM timestamp_column)
           - DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
        
        3. **Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Î≥ÄÌôò**:
           - CAST(column AS STRING/INT64/FLOAT64)
           - SAFE_CAST(column AS type) -- Ïò§Î•ò Î∞©ÏßÄ
        
        4. **Î¨∏ÏûêÏó¥ Ï≤òÎ¶¨**:
           - CONCAT(str1, str2) ÎòêÎäî str1 || str2
           - REGEXP_CONTAINS(text, pattern)
           - SPLIT(text, delimiter)
        
        5. **Î∞∞Ïó¥/Íµ¨Ï°∞Ï≤¥ Ï≤òÎ¶¨**:
           - UNNEST(array_column)
           - array_column[OFFSET(0)] -- Ï≤´ Î≤àÏß∏ ÏöîÏÜå
        
        **Í∏àÏßÄÎêú Î¨∏Î≤ï (MySQL/PostgreSQL):**
        - SHOW TABLES, SHOW COLUMNS, DESCRIBE (‚Üí INFORMATION_SCHEMA ÏÇ¨Ïö©)
        - LIMIT offset, count (‚Üí LIMIT count OFFSET offset)
        - Î∞±Ìã± ÏóÜÎäî ÌÖåÏù¥Î∏îÎ™Ö (‚Üí Î∞òÎìúÏãú Î∞±Ìã± ÏÇ¨Ïö©)
        - DATE_FORMAT (‚Üí FORMAT_DATE ÏÇ¨Ïö©)
        
        **ÏÑ±Îä• ÏµúÏ†ÅÌôî Ï†ÑÎûµ:**
        - WHERE Ï†àÏóêÏÑú ÌååÌã∞ÏÖò Ïª¨Îüº(ÎÇ†Ïßú) Ïö∞ÏÑ† ÌïÑÌÑ∞ÎßÅ
        - SELECT *Î≥¥Îã§ ÌïÑÏöîÌïú Ïª¨ÎüºÎßå Ï°∞Ìöå
        - ÌÅ∞ ÌÖåÏù¥Î∏î JOIN Ïãú ÏûëÏùÄ ÌÖåÏù¥Î∏îÏùÑ Î®ºÏ†Ä ÌïÑÌÑ∞ÎßÅ
        - ÏúàÎèÑÏö∞ Ìï®ÏàòÎ≥¥Îã§ ÏßëÍ≥Ñ Ìï®Ïàò Ïö∞ÏÑ† Í≥†Î†§
        - ÏÑúÎ∏åÏøºÎ¶¨Î≥¥Îã§ WITH Ï†à(CTE) ÏÇ¨Ïö© Í∂åÏû•
        
        **Îç∞Ïù¥ÌÑ∞ÏÖã Í≤ΩÎ°ú Í∑úÏπô:**
        - ÌîÑÎ°úÏ†ùÌä∏Í∞Ä us-all-dataÏù∏ Í≤ΩÏö∞: `us-all-data.us_plus.table_name`
        - Ìï≠ÏÉÅ Î∞±Ìã±ÏúºÎ°ú Í∞êÏã∏ÏÑú ÏÇ¨Ïö©
        - INFORMATION_SCHEMA Ï°∞ÌöåÏãúÏóêÎèÑ Ï†ÑÏ≤¥ Í≤ΩÎ°ú ÏÇ¨Ïö©
        
        **ÌíàÏßà Î≥¥Ïû•:**
        - BigQuery Î¨∏Î≤ï Ï§ÄÏàòÏú®: 100% Î™©Ìëú
        - ÏøºÎ¶¨ ÏÑ±Îä•: Ïä§Ï∫î Îç∞Ïù¥ÌÑ∞Îüâ ÏµúÏÜåÌôî
        - ÎπÑÏö© Ìö®Ïú®ÏÑ±: Ï†ÅÏ†àÌïú LIMITÍ≥º WHERE Ï°∞Í±¥ ÏÇ¨Ïö©
        """
    
    async def process_message(self, message: AgentMessage) -> AgentMessage:
        """Î©îÏãúÏßÄ Ï≤òÎ¶¨ - SQL ÏÉùÏÑ± ÏûëÏóÖ ÏàòÌñâ"""
        try:
            # ÏûÖÎ†• Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù
            if not await self.validate_input(message):
                return self.create_error_message(message, ValueError("Invalid input message"))
            
            # Î©îÏãúÏßÄ ÌûàÏä§ÌÜ†Î¶¨Ïóê Ï∂îÍ∞Ä
            self.add_message_to_history(message)
            
            # ÏûëÏóÖ ÌÉÄÏûÖÏóê Îî∞Î•∏ Ï≤òÎ¶¨
            task_type = message.content.get("task_type", "generate_sql")
            input_data = message.content.get("input_data", {})
            
            if task_type == "generate_sql":
                result = await self._optimized_generation(input_data)
            elif task_type == "execute_with_improvements":
                result = await self._execute_with_improvements(input_data)
            else:
                result = await self._optimized_generation(input_data)  # Í∏∞Î≥∏Í∞í
            
            # ÏÑ±Í≥µ ÏùëÎãµ ÏÉùÏÑ±
            return self.create_response_message(message, result)
            
        except Exception as e:
            logger.error(f"SqlGenerator Agent processing failed: {str(e)}")
            return self.create_error_message(message, e)
    

    
    async def _optimized_generation(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ÏµúÏ†ÅÌôîÎêú SQL ÏÉùÏÑ± Î∞è Ïã§Ìñâ"""
        query = input_data.get("query", "")
        analysis_result = input_data.get("analysis_result", {})
        exploration_result = input_data.get("exploration_result", {})
        
        logger.info(f"SqlGenerator: Optimized generation started")
        
        # Ï†ÑÎã¨Î∞õÏùÄ RAG Ïª®ÌÖçÏä§Ìä∏ ÏÇ¨Ïö© (Ï§ëÎ≥µ Ìò∏Ï∂ú Î∞©ÏßÄ)
        relevant_context = input_data.get("rag_context")
        if not relevant_context:
            # Ï†ÑÎã¨Î∞õÏùÄ Ïª®ÌÖçÏä§Ìä∏Í∞Ä ÏóÜÎäî Í≤ΩÏö∞ÏóêÎßå RAG Ìò∏Ï∂ú
            try:
                relevant_context = schema_retriever.create_context_summary(query, max_tables=5)
                logger.info("RAG context not provided, performing fresh RAG search")
            except Exception as e:
                logger.warning(f"RAG search failed: {str(e)}")
                relevant_context = "Ïä§ÌÇ§Îßà Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§."
        else:
            logger.info("Using pre-fetched RAG context from SchemaAnalyzer")
        
        # ÌÉêÏÉâ Í≤∞Í≥º Ïª®ÌÖçÏä§Ìä∏ ÏÉùÏÑ±
        exploration_context = self._build_exploration_context(exploration_result)
        
        # Î∂ÑÏÑù Í≤∞Í≥º Ïª®ÌÖçÏä§Ìä∏ ÏÉùÏÑ±
        analysis_context = self._build_analysis_context(analysis_result)
        
        # Îç∞Ïù¥ÌÑ∞ÏÖã Í≤ΩÎ°ú Ï†ïÎ≥¥ Ï∂îÍ∞Ä
        dataset_info = ""
        if bq_client.full_dataset_path:
            dataset_info = f"""
        **Îç∞Ïù¥ÌÑ∞ÏÖã Í≤ΩÎ°ú Ï†ïÎ≥¥:**
        - Í∏∞Î≥∏ Í≤ΩÎ°ú: {bq_client.full_dataset_path}
        - ÌÖåÏù¥Î∏î ÏÇ¨Ïö©Ïãú: `{bq_client.full_dataset_path}.table_name` ÌòïÏãù ÏÇ¨Ïö©
        - INFORMATION_SCHEMA: `{bq_client.full_dataset_path}.INFORMATION_SCHEMA.COLUMNS`
        """
        
        # ÏµúÏ†ÅÌôîÎêú SQL ÏÉùÏÑ± ÌîÑÎ°¨ÌîÑÌä∏
        user_message = f"""
        ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠: {query}
        
        Ïä§ÌÇ§Îßà Ï†ïÎ≥¥:
        {relevant_context}
        
        {analysis_context}
        
        {exploration_context}
        
        {dataset_info}
        
        ÏúÑ Ï†ïÎ≥¥Î•º Ï¢ÖÌï©ÌïòÏó¨ ÏÑ±Îä• ÏµúÏ†ÅÌôîÎêú BigQuery Standard SQL ÏøºÎ¶¨Î•º ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
        
        **BigQuery ÏµúÏ†ÅÌôî Í≥†Î†§ÏÇ¨Ìï≠:**
        1. ÌÖåÏù¥Î∏îÎ™ÖÏùÄ Î∞òÎìúÏãú Î∞±Ìã±Í≥º ÏôÑÏ†ÑÌïú Í≤ΩÎ°ú ÏÇ¨Ïö©: `project.dataset.table`
        2. WHERE Ï†àÏóêÏÑú ÌååÌã∞ÏÖò Ïª¨Îüº(ÎÇ†Ïßú) Ïö∞ÏÑ† ÌïÑÌÑ∞ÎßÅ
        3. Ï†ÅÏ†àÌïú JOIN ÏàúÏÑú (ÏûëÏùÄ ÌÖåÏù¥Î∏î Î®ºÏ†Ä)
        4. Î∂àÌïÑÏöîÌïú Ïª¨Îüº Ï°∞Ìöå ÏµúÏÜåÌôî (SELECT * ÏßÄÏñë)
        5. LIMIT ÏÇ¨Ïö©ÏúºÎ°ú Í≤∞Í≥º Ï†úÌïú (Í∏∞Î≥∏ 100)
        6. INFORMATION_SCHEMA Ï°∞ÌöåÏãúÏóêÎèÑ ÏôÑÏ†ÑÌïú Í≤ΩÎ°ú ÏÇ¨Ïö©
        
        **Í∏àÏßÄÏÇ¨Ìï≠:**
        - MySQL/PostgreSQL Î¨∏Î≤ï Ï†àÎåÄ ÏÇ¨Ïö© Í∏àÏßÄ
        - SHOW, DESCRIBE Îì± MySQL Î¨∏Î≤ï
        - Î∞±Ìã± ÏóÜÎäî ÌÖåÏù¥Î∏îÎ™Ö
        
        BigQuery Standard SQL ÏøºÎ¶¨Îßå Î∞òÌôòÌïòÏÑ∏Ïöî.
        """
        
        try:
            start_time = datetime.now()
            response_content = await self.send_llm_request(user_message)
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # SQL Ï†ïÎ¶¨ Î∞è Í≤ÄÏ¶ù
            sql_query = self._clean_sql_response(response_content)
            complexity = self._assess_query_complexity(sql_query)
            optimizations = self._detect_applied_optimizations(sql_query)
            
            # ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
            if complexity in [QueryComplexity.COMPLEX, QueryComplexity.ADVANCED]:
                self.performance_stats["complex_queries"] += 1
            if optimizations:
                self.performance_stats["optimization_applied"] += 1
            
            self._update_generation_stats(processing_time)
            
            # SQL Ïã§Ìñâ Ï∂îÍ∞Ä
            print(f"üîÑ SQL Ïã§Ìñâ Ï§ë...")
            print(f"üìù SQL: {sql_query}")
            
            try:
                query_result = bq_client.execute_query(sql_query)
                execution_time = (datetime.now() - start_time).total_seconds()
                
                if query_result["success"]:
                    print(f"‚úÖ SQL Ïã§Ìñâ ÏÑ±Í≥µ! ({execution_time:.2f}Ï¥à)")
                    print(f"üìä Í≤∞Í≥º: {query_result['returned_rows']}Í∞ú Ìñâ Î∞òÌôò")
                    
                    # Ïã§Ìñâ Í≤∞Í≥º ÏÉÅÏÑ∏ Ï∂úÎ†•
                    self._print_query_results(query_result)
                    
                    result = {
                        "generation_type": "optimized_generation",
                        "sql_query": sql_query,
                        "processing_time": processing_time,
                        "execution_time": execution_time,
                        "complexity": complexity,
                        "optimization_applied": len(optimizations) > 0,
                        "applied_optimizations": optimizations,
                        "schema_context_used": relevant_context is not None,
                        "exploration_used": bool(exploration_result),
                        "confidence": self._calculate_confidence(sql_query, analysis_result),
                        "query_result": query_result,  # ‚Üê Ïã§Ìñâ Í≤∞Í≥º Ï∂îÍ∞Ä
                        "success": True
                    }
                else:
                    print(f"‚ùå SQL Ïã§Ìñâ Ïã§Ìå®: {query_result.get('error', 'Unknown error')}")
                    
                    result = {
                        "generation_type": "optimized_generation",
                        "sql_query": sql_query,
                        "processing_time": processing_time,
                        "complexity": complexity,
                        "optimization_applied": len(optimizations) > 0,
                        "applied_optimizations": optimizations,
                        "schema_context_used": relevant_context is not None,
                        "exploration_used": bool(exploration_result),
                        "confidence": self._calculate_confidence(sql_query, analysis_result),
                        "query_result": query_result,  # ‚Üê Ïã§Ìñâ Í≤∞Í≥º Ï∂îÍ∞Ä
                        "success": False,
                        "error": query_result.get('error', 'Unknown error')
                    }
                
            except Exception as e:
                print(f"‚ùå SQL Ïã§Ìñâ Ï§ë Ïò§Î•ò: {str(e)}")
                
                result = {
                    "generation_type": "optimized_generation",
                    "sql_query": sql_query,
                    "processing_time": processing_time,
                    "complexity": complexity,
                    "optimization_applied": len(optimizations) > 0,
                    "applied_optimizations": optimizations,
                    "schema_context_used": relevant_context is not None,
                    "exploration_used": bool(exploration_result),
                    "confidence": self._calculate_confidence(sql_query, analysis_result),
                    "success": False,
                    "error": str(e)
                }
            
            # ÏÉùÏÑ± ÌûàÏä§ÌÜ†Î¶¨Ïóê Ï∂îÍ∞Ä
            self._add_to_generation_history(query, result)
            
            logger.info(f"Optimized generation completed in {processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"Optimized generation failed: {str(e)}")
            return self._create_fallback_result("optimized_generation", str(e))
    

    

    
    def _build_exploration_context(self, exploration_result: Dict) -> str:
        """ÌÉêÏÉâ Í≤∞Í≥ºÎ•º Ïª®ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôò"""
        if not exploration_result or not exploration_result.get("insights"):
            return ""
        
        insights = exploration_result.get("insights", [])
        return f"""
=== ÌÉêÏÉâÏùÑ ÌÜµÌï¥ Î∞úÍ≤¨Îêú Ï†ïÎ≥¥ ===
{chr(10).join([f"- {insight}" for insight in insights])}

Ïù¥ Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú Îçî Ï†ïÌôïÌïú SQL ÏøºÎ¶¨Î•º ÏÉùÏÑ±ÌïòÏÑ∏Ïöî.
        """
    
    def _build_analysis_context(self, analysis_result: Dict) -> str:
        """Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Ïª®ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôò"""
        if not analysis_result:
            return ""
        
        uncertainties = analysis_result.get("uncertainties", [])
        if not uncertainties:
            return ""
        
        context = "=== Î∂àÌôïÏã§ÏÑ± Î∂ÑÏÑù Í≤∞Í≥º ===\n"
        for uncertainty in uncertainties:
            context += f"- {uncertainty.get('type', 'unknown')}: {uncertainty.get('description', 'N/A')}\n"
        
        context += "\nÏù¥Îü¨Ìïú Î∂àÌôïÏã§ÏÑ±ÏùÑ Í≥†Î†§ÌïòÏó¨ Ï†ÅÏ†àÌïú Í∞ÄÏ†ïÏùÑ ÏÑ∏Ïö∞Í≥† SQLÏùÑ ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.\n"
        return context
    
    def _clean_sql_response(self, response_content: str) -> str:
        """SQL ÏùëÎãµ Ï†ïÎ¶¨ - ÏΩîÎìú Î∏îÎ°ùÏóêÏÑú SQLÎßå Ï∂îÏ∂ú"""
        # import reÎäî ÌååÏùº ÏÉÅÎã®ÏóêÏÑú Ïù¥ÎØ∏ ÌñàÏúºÎØÄÎ°ú Ï†úÍ±∞
        
        # ```sql ... ``` Ìå®ÌÑ¥ Ï∞æÍ∏∞
        sql_pattern = r'```sql\s*(.*?)\s*```'
        match = re.search(sql_pattern, response_content, re.DOTALL | re.IGNORECASE)
        
        if match:
            # ÏΩîÎìú Î∏îÎ°ù ÏïàÏùò SQL Ï∂îÏ∂ú
            sql_query = match.group(1).strip()
            return sql_query
        
        # ``` ... ``` Ìå®ÌÑ¥ Ï∞æÍ∏∞ (sql ÏóÜÏù¥)
        code_pattern = r'```\s*(.*?)\s*```'
        match = re.search(code_pattern, response_content, re.DOTALL)
        
        if match:
            # ÏΩîÎìú Î∏îÎ°ù ÏïàÏùò ÎÇ¥Ïö© Ï∂îÏ∂ú
            sql_query = match.group(1).strip()
            return sql_query
        
        # ÏΩîÎìú Î∏îÎ°ùÏù¥ ÏóÜÏúºÎ©¥ Ï†ÑÏ≤¥ ÎÇ¥Ïö©ÏóêÏÑú SQL Ï∂îÏ∂ú
        # SELECTÎ°ú ÏãúÏûëÌïòÎäî Ï≤´ Î≤àÏß∏ ÎùºÏù∏Î∂ÄÌÑ∞ Ï∞æÍ∏∞
        lines = response_content.split('\n')
        sql_lines = []
        found_sql = False
        
        for line in lines:
            line_stripped = line.strip()
            if not line_stripped:
                continue
                
            # SQL ÏãúÏûë ÌÇ§ÏõåÎìú Í∞êÏßÄ
            if line_stripped.upper().startswith(('SELECT', 'WITH', 'CREATE', 'INSERT', 'UPDATE', 'DELETE')):
                found_sql = True
                sql_lines.append(line)
            elif found_sql:
                sql_lines.append(line)
        
        if sql_lines:
            return '\n'.join(sql_lines).strip()
        
        # ÏïÑÎ¨¥Í≤ÉÎèÑ Ï∞æÏßÄ Î™ªÌïòÎ©¥ ÏõêÎ≥∏ Î∞òÌôò
        return response_content.strip()
    
    def _assess_query_complexity(self, sql_query: str) -> str:
        """ÏøºÎ¶¨ Î≥µÏû°ÎèÑ ÌèâÍ∞Ä"""
        sql_lower = sql_query.lower()
        
        # Í≥†Í∏â Í∏∞Îä• Í≤ÄÏ∂ú
        advanced_patterns = ["window", "partition by", "row_number", "rank", "cte", "with recursive"]
        if any(pattern in sql_lower for pattern in advanced_patterns):
            return QueryComplexity.ADVANCED
        
        # Î≥µÏû°Ìïú Í∏∞Îä• Í≤ÄÏ∂ú
        complex_patterns = ["subquery", "exists", "case when", "union", "having"]
        subquery_count = len(re.findall(r'\bselect\b', sql_lower)) - 1
        if subquery_count > 0 or any(pattern in sql_lower for pattern in complex_patterns):
            return QueryComplexity.COMPLEX
        
        # Ï§ëÍ∞Ñ Î≥µÏû°ÎèÑ Í≤ÄÏ∂ú
        moderate_patterns = ["join", "group by", "order by", "distinct"]
        if any(pattern in sql_lower for pattern in moderate_patterns):
            return QueryComplexity.MODERATE
        
        return QueryComplexity.SIMPLE
    
    
    
    def _calculate_confidence(self, sql_query: str, analysis_result: Dict) -> float:
        """ÏÉùÏÑ±Îêú SQLÏùò Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞"""
        confidence = 0.8  # Í∏∞Î≥∏ Ïã†Î¢∞ÎèÑ
        
        # Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ ÌôúÏö© Ïó¨Î∂Ä
        if bq_client.schema_info:
            confidence += 0.1
        
        # Î∂àÌôïÏã§ÏÑ± Ìï¥Í≤∞ Ïó¨Î∂Ä
        uncertainties = analysis_result.get("uncertainties", []) if analysis_result else []
        if not uncertainties:
            confidence += 0.1
        elif len(uncertainties) > 3:
            confidence -= 0.2
        
        # SQL Î≥µÏû°ÎèÑÏóê Îî∞Î•∏ Ï°∞Ï†ï
        complexity = self._assess_query_complexity(sql_query)
        if complexity == QueryComplexity.SIMPLE:
            confidence += 0.05
        elif complexity == QueryComplexity.ADVANCED:
            confidence -= 0.1
        
        return min(max(confidence, 0.0), 1.0)
    

    
    
    async def _add_conditions_and_filters(self, sql_with_joins: str, original_query: str) -> str:
        """Ï°∞Í±¥ Î∞è ÌïÑÌÑ∞ Ï∂îÍ∞Ä"""
        prompt = f"""
        ÌòÑÏû¨ SQL: {sql_with_joins}
        ÏõêÎ≥∏ ÏöîÏ≤≠: {original_query}
        
        ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠Ïóê ÎßûÎäî WHERE Ï°∞Í±¥, ORDER BY, LIMIT Îì±ÏùÑ Ï∂îÍ∞ÄÌï¥Ï£ºÏÑ∏Ïöî.
        
        ÏôÑÏÑ±Îêú SQL ÏøºÎ¶¨Îßå Î∞òÌôòÌïòÏÑ∏Ïöî.
        """
        
        response = await self.send_llm_request(prompt)
        return self._clean_sql_response(response)
    
    async def _execute_with_improvements(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """SQL Ïã§Ìñâ Î∞è Ïã§Ìå®Ïãú Í∞úÏÑ†Î∞©Ïïà Ï¶âÏãú Ï†ÅÏö©"""
        sql_query = input_data.get("sql_query", "")
        original_query = input_data.get("original_query", "")
        
        logger.info("SqlGenerator: Execute with improvements started")
        
        if not sql_query:
            return {
                "execution_type": "execute_with_improvements",
                "success": False,
                "error": "Ïã§ÌñâÌï† SQL ÏøºÎ¶¨Í∞Ä ÏóÜÏäµÎãàÎã§.",
                "sql_query": sql_query
            }
        
        start_time = datetime.now()
        
        # 1Îã®Í≥Ñ: ÏõêÎ≥∏ SQL Ïã§Ìñâ ÏãúÎèÑ
        print(f"üîÑ SQL Ïã§Ìñâ Ï§ë...")
        print(f"üìù SQL: {sql_query}")
        
        try:
            query_result = bq_client.execute_query(sql_query)
            processing_time = (datetime.now() - start_time).total_seconds()
            
            if query_result["success"]:
                # ÏÑ±Í≥µÏãú Î∞îÎ°ú Î∞òÌôò
                print(f"‚úÖ SQL Ïã§Ìñâ ÏÑ±Í≥µ! ({processing_time:.2f}Ï¥à)")
                print(f"üìä Í≤∞Í≥º: {query_result['returned_rows']}Í∞ú Ìñâ Î∞òÌôò")
                
                # Ïã§Ìñâ Í≤∞Í≥º ÏÉÅÏÑ∏ Ï∂úÎ†•
                self._print_query_results(query_result)
                
                return {
                    "execution_type": "execute_with_improvements",
                    "success": True,
                    "sql_query": sql_query,
                    "query_result": query_result,
                    "processing_time": processing_time,
                    "improvements_applied": False
                }
            
            # 2Îã®Í≥Ñ: Ïã§Ìå®Ïãú Í∞úÏÑ†Î∞©Ïïà ÏÉùÏÑ±
            print(f"‚ùå SQL Ïã§Ìñâ Ïã§Ìå®: {query_result.get('error', 'Unknown error')}")
            print("üîß Í∞úÏÑ†Î∞©Ïïà ÏÉùÏÑ± Ï§ë...")
            
            improvements = await self._generate_sql_improvements(sql_query, query_result.get('error', ''), original_query)
            
            if not improvements:
                return {
                    "execution_type": "execute_with_improvements", 
                    "success": False,
                    "sql_query": sql_query,
                    "error": query_result.get('error', ''),
                    "improvements_generated": False
                }
            
            # 3Îã®Í≥Ñ: Í∞úÏÑ†Î∞©Ïïà Ï∂úÎ†• Î∞è ÏÇ¨Ïö©Ïûê ÌôïÏù∏
            print("\nüõ†Ô∏è Ï†úÏïàÎêú Í∞úÏÑ†Î∞©Ïïà:")
            for i, improvement in enumerate(improvements, 1):
                print(f"{i}. {improvement['description']}")
                if improvement.get('improved_sql'):
                    print(f"   Í∞úÏÑ†Îêú SQL: {improvement['improved_sql'][:100]}...")
            
            # 4Îã®Í≥Ñ: ÏûêÎèô Ïã§Ìñâ (Í∞ÄÏû• Ïã†Î¢∞ÎèÑ ÎÜíÏùÄ Í∞úÏÑ†Ïïà)
            best_improvement = max(improvements, key=lambda x: x.get('confidence', 0))
            
            if best_improvement.get('confidence', 0) > 0.7:
                print(f"\nüöÄ Ïã†Î¢∞ÎèÑ ÎÜíÏùÄ Í∞úÏÑ†ÏïàÏùÑ ÏûêÎèô Ïã§ÌñâÌï©ÎãàÎã§. (Ïã†Î¢∞ÎèÑ: {best_improvement['confidence']:.2f})")
                return await self._execute_improved_sql(best_improvement, start_time)
            else:
                # Ïã†Î¢∞ÎèÑÍ∞Ä ÎÇÆÏúºÎ©¥ ÏÇ¨Ïö©Ïûê ÌôïÏù∏ ÏöîÏ≤≠
                if await self._ask_user_confirmation_async():
                    return await self._execute_improved_sql(best_improvement, start_time)
                else:
                    print("‚ùå ÏÇ¨Ïö©ÏûêÍ∞Ä Í∞úÏÑ†Î∞©Ïïà Ïã§ÌñâÏùÑ Ï∑®ÏÜåÌñàÏäµÎãàÎã§.")
                    return {
                        "execution_type": "execute_with_improvements",
                        "success": False,
                        "sql_query": sql_query,
                        "error": query_result.get('error', ''),
                        "improvements_generated": True,
                        "improvements_applied": False,
                        "user_cancelled": True
                    }
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"Execute with improvements failed: {str(e)}")
            return {
                "execution_type": "execute_with_improvements",
                "success": False,
                "sql_query": sql_query,
                "error": str(e),
                "processing_time": processing_time
            }
    
    async def _generate_sql_improvements(self, sql_query: str, error_message: str, original_query: str) -> List[Dict[str, Any]]:
        """SQL Ïò§Î•ò Î∂ÑÏÑù Î∞è Í∞úÏÑ†Î∞©Ïïà ÏÉùÏÑ±"""
        
        # Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï§ÄÎπÑ
        schema_context = self._build_schema_context_for_improvement(sql_query)
        
        system_prompt = f"""
        ÎãπÏã†ÏùÄ BigQuery SQL Ïò§Î•ò Î∂ÑÏÑù Î∞è Í∞úÏÑ† Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§.
        
        **Î∂ÑÏÑùÌï† Ï†ïÎ≥¥:**
        - ÏõêÎ≥∏ ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠: {original_query}
        - Ïã§Ìå®Ìïú SQL: {sql_query}
        - Ïò§Î•ò Î©îÏãúÏßÄ: {error_message}
        
        **Ïä§ÌÇ§Îßà Ï†ïÎ≥¥:**
        {schema_context}
        
        **Í∞úÏÑ† Ï†ÑÎûµ:**
        1. Ïª¨ÎüºÎ™Ö Ïò§Î•ò: Ï†ïÌôïÌïú Ïª¨ÎüºÎ™ÖÏúºÎ°ú ÏàòÏ†ï (Ïò§Î•ò Î©îÏãúÏßÄÏùò "Did you mean" ÌôúÏö©)
        2. Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Ïò§Î•ò: PARSE_TIMESTAMP, CAST Îì± Ï†ÅÏ†àÌïú ÌÉÄÏûÖ Î≥ÄÌôò
        3. ÌÖåÏù¥Î∏îÎ™Ö Ïò§Î•ò: Ïò¨Î∞îÎ•∏ dataset.table ÌòïÏãùÏúºÎ°ú ÏàòÏ†ï
        4. Î¨∏Î≤ï Ïò§Î•ò: BigQuery ÌëúÏ§Ä SQL Î¨∏Î≤ï Ï§ÄÏàò
        5. Ìï®Ïàò ÏÇ¨Ïö© Ïò§Î•ò: Ïò¨Î∞îÎ•∏ Ìï®Ïàò Î∞è ÌååÎùºÎØ∏ÌÑ∞
        
        **ÏùëÎãµ ÌòïÏãù (JSON):**
        {{
            "improvements": [
                {{
                    "issue_type": "column_name|data_type|table_name|syntax|function",
                    "description": "Íµ¨Ï≤¥Ï†ÅÏù∏ Î¨∏Ï†úÏ†êÍ≥º Ìï¥Í≤∞Ï±Ö ÏÑ§Î™Ö",
                    "improved_sql": "ÏôÑÏ†ÑÌûà ÏàòÏ†ïÎêú SQL ÏøºÎ¶¨",
                    "confidence": 0.0-1.0,
                    "changes_made": ["Î≥ÄÍ≤ΩÏÇ¨Ìï≠1", "Î≥ÄÍ≤ΩÏÇ¨Ìï≠2"]
                }}
            ]
        }}
        """
        
        try:
            response_content = await self.send_llm_request(system_prompt)
            
            # JSON ÌååÏã±
            content = response_content.strip()
            if content.startswith("```json"):
                content = content[7:]
            if content.startswith("```"):
                content = content[3:]
            if content.endswith("```"):
                content = content[:-3]
            
            improvement_data = json.loads(content.strip())
            improvements = improvement_data.get("improvements", [])
            
            # Í∏∞Î≥∏ Í∞úÏÑ†Î∞©Ïïà Ï∂îÍ∞Ä (AIÍ∞Ä ÎÜìÏπú Î∂ÄÎ∂Ñ Î≥¥ÏôÑ)
            basic_improvements = self._generate_basic_improvements(sql_query, error_message)
            improvements.extend(basic_improvements)
            
            return improvements
            
        except Exception as e:
            logger.error(f"AI improvement generation failed: {str(e)}")
            # AI Ïã§Ìå®Ïãú Í∏∞Î≥∏ Í∞úÏÑ†Î∞©ÏïàÎßå Î∞òÌôò
            return self._generate_basic_improvements(sql_query, error_message)
    
    def _generate_basic_improvements(self, sql_query: str, error_message: str) -> List[Dict[str, Any]]:
        """Í∏∞Î≥∏Ï†ÅÏù∏ Í∞úÏÑ†Î∞©Ïïà ÏÉùÏÑ± (Ìå®ÌÑ¥ Í∏∞Î∞ò)"""
        improvements = []
        
        # 1. Ïª¨ÎüºÎ™Ö Ïò§Î•ò Ï≤òÎ¶¨
        if "Unrecognized name" in error_message:
            match = re.search(r"Unrecognized name: (\w+)", error_message)
            suggestion_match = re.search(r"Did you mean (\w+)?", error_message)
            
            if match and suggestion_match:
                wrong_column = match.group(1)
                correct_column = suggestion_match.group(1)
                improved_sql = sql_query.replace(wrong_column, correct_column)
                
                improvements.append({
                    "issue_type": "column_name",
                    "description": f"Ïª¨ÎüºÎ™Ö '{wrong_column}'ÏùÑ '{correct_column}'ÏúºÎ°ú ÏàòÏ†ï",
                    "improved_sql": improved_sql,
                    "confidence": 0.95,
                    "changes_made": [f"{wrong_column} ‚Üí {correct_column}"]
                })
        
        # 2. Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Ïò§Î•ò Ï≤òÎ¶¨ 
        elif "No matching signature" in error_message and ("TIMESTAMP" in error_message or "STRING" in error_message):
            if "createdAt" in sql_query:
                # createdAt Ïª¨ÎüºÏùÑ TIMESTAMPÎ°ú Î≥ÄÌôò (ISO 8601 ÌòïÏãù)
                improved_sql = re.sub(
                    r"(\w+\.)?createdAt(\s*[><=]+\s*)",
                    r"PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*SZ', \1createdAt)\2",
                    sql_query
                )
                
                improvements.append({
                    "issue_type": "data_type",
                    "description": "createdAt Ïª¨ÎüºÏùÑ ISO 8601 ÌòïÏãùÏùò PARSE_TIMESTAMPÎ°ú Î≥ÄÌôòÌïòÏó¨ ÎÇ†Ïßú ÎπÑÍµê Í∞ÄÎä•ÌïòÎèÑÎ°ù ÏàòÏ†ï",
                    "improved_sql": improved_sql,
                    "confidence": 0.9,
                    "changes_made": ["createdAt ‚Üí PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*SZ', createdAt)"]
                })
        
        # 3. Ìï®Ïàò Ïò§Î•ò Ï≤òÎ¶¨
        elif "CURRENT_DATE" in sql_query and "INTERVAL" in error_message:
            # CURRENT_DATE() ‚Üí CURRENT_TIMESTAMP()Î°ú ÏàòÏ†ï
            improved_sql = sql_query.replace("CURRENT_DATE()", "CURRENT_TIMESTAMP()")
            
            improvements.append({
                "issue_type": "function",
                "description": "ÎÇ†Ïßú Ìï®ÏàòÎ•º CURRENT_TIMESTAMP()Î°ú ÏàòÏ†ï",
                "improved_sql": improved_sql,
                "confidence": 0.8,
                "changes_made": ["CURRENT_DATE() ‚Üí CURRENT_TIMESTAMP()"]
            })
        
        return improvements
    
    def _build_schema_context_for_improvement(self, sql_query: str) -> str:
        """Í∞úÏÑ†ÏùÑ ÏúÑÌïú Ïä§ÌÇ§Îßà Ïª®ÌÖçÏä§Ìä∏ ÏÉùÏÑ±"""
        try:
            schema_info = getattr(bq_client, 'schema_info', [])
            if not schema_info:
                return "Ïä§ÌÇ§Îßà Ï†ïÎ≥¥Í∞Ä ÏóÜÏäµÎãàÎã§."
            
            # SQLÏóêÏÑú Ïñ∏Í∏âÎêú ÌÖåÏù¥Î∏î Ï∞æÍ∏∞
            mentioned_tables = []
            for table_info in schema_info:
                # schema_infoÍ∞Ä Î¨∏ÏûêÏó¥Ïù∏ Í≤ΩÏö∞ Ï≤òÎ¶¨
                if isinstance(table_info, str):
                    continue
                    
                table_name = table_info.get("table_name", "") if isinstance(table_info, dict) else ""
                if table_name and table_name.lower() in sql_query.lower():
                    mentioned_tables.append(table_info)
            
            if not mentioned_tables:
                return "Í¥ÄÎ†® ÌÖåÏù¥Î∏îÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
            
            context = "Í¥ÄÎ†® ÌÖåÏù¥Î∏î Ïä§ÌÇ§Îßà:\n"
            for table in mentioned_tables[:2]:  # ÏµúÎåÄ 2Í∞ú
                if not isinstance(table, dict):
                    continue
                    
                table_name = table.get("table_name", "")
                columns = table.get("columns", [])
                
                context += f"\nÌÖåÏù¥Î∏î: {table_name}\n"
                for col in columns[:8]:  # ÏµúÎåÄ 8Í∞ú Ïª¨Îüº
                    if isinstance(col, dict):
                        col_name = col.get("column_name", "")
                        col_type = col.get("data_type", "")
                        context += f"  - {col_name} ({col_type})\n"
            
            return context
            
        except Exception as e:
            logger.error(f"Schema context building failed: {str(e)}")
            return "Ïä§ÌÇ§Îßà Ï†ïÎ≥¥ Ï≤òÎ¶¨ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§."
    
    async def _ask_user_confirmation_async(self) -> bool:
        """ÏÇ¨Ïö©Ïûê ÌôïÏù∏ (ÎπÑÎèôÍ∏∞)"""
        try:
            print("\n‚ùì Í∞úÏÑ†Îêú ÏøºÎ¶¨Î•º Ïã§ÌñâÌïòÏãúÍ≤†ÏäµÎãàÍπå?")
            print("   y/yes - Ïã§Ìñâ")
            print("   n/no - Ï∑®ÏÜå")
            
            # Ïã§Ï†ú ÌîÑÎ°úÎçïÏÖòÏóêÏÑúÎäî Ï†ÅÏ†àÌïú ÎπÑÎèôÍ∏∞ ÏûÖÎ†• Ï≤òÎ¶¨ ÌïÑÏöî
            # ÌòÑÏû¨Îäî Í∞ÑÎã®Ìïú ÎèôÍ∏∞ Ï≤òÎ¶¨
            import asyncio
            
            def get_input():
                return input("\nÏÑ†ÌÉùÌïòÏÑ∏Ïöî (y/n): ").strip().lower()
            
            response = await asyncio.get_event_loop().run_in_executor(None, get_input)
            return response in ['y', 'yes', 'Ïòà', 'ÎÑ§']
            
        except Exception as e:
            logger.error(f"User confirmation failed: {str(e)}")
            return False
    
    async def _execute_improved_sql(self, improvement: Dict, start_time: datetime) -> Dict[str, Any]:
        """Í∞úÏÑ†Îêú SQL Ïã§Ìñâ"""
        improved_sql = improvement.get('improved_sql', '')
        
        if not improved_sql:
            return {
                "execution_type": "execute_with_improvements",
                "success": False,
                "error": "Í∞úÏÑ†Îêú SQLÏù¥ ÏóÜÏäµÎãàÎã§."
            }
        
        print(f"\nüîÑ Í∞úÏÑ†Îêú ÏøºÎ¶¨ Ïã§Ìñâ Ï§ë...")
        print(f"üìù Í∞úÏÑ†Îêú SQL: {improved_sql}")
        print(f"üõ†Ô∏è Ï†ÅÏö©Îêú Í∞úÏÑ†ÏÇ¨Ìï≠: {improvement.get('description', '')}")
        
        try:
            query_result = bq_client.execute_query(improved_sql)
            processing_time = (datetime.now() - start_time).total_seconds()
            
            if query_result["success"]:
                print(f"‚úÖ Í∞úÏÑ†Îêú ÏøºÎ¶¨ Ïã§Ìñâ ÏÑ±Í≥µ! ({processing_time:.2f}Ï¥à)")
                print(f"üìä Í≤∞Í≥º: {query_result['returned_rows']}Í∞ú Ìñâ Î∞òÌôò")
                
                # Ïã§Ìñâ Í≤∞Í≥º ÏÉÅÏÑ∏ Ï∂úÎ†•
                self._print_query_results(query_result)
                
                # ÏÑ±Í≥µ ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
                self.performance_stats["optimization_applied"] += 1
                
                return {
                    "execution_type": "execute_with_improvements",
                    "success": True,
                    "sql_query": improved_sql,
                    "original_sql": improvement.get('original_sql', ''),
                    "query_result": query_result,
                    "processing_time": processing_time,
                    "improvements_applied": True,
                    "improvement_details": {
                        "type": improvement.get('issue_type', ''),
                        "description": improvement.get('description', ''),
                        "confidence": improvement.get('confidence', 0),
                        "changes_made": improvement.get('changes_made', [])
                    }
                }
            else:
                print(f"‚ùå Í∞úÏÑ†Îêú ÏøºÎ¶¨ÎèÑ Ïã§Ìñâ Ïã§Ìå®: {query_result.get('error', '')}")
                return {
                    "execution_type": "execute_with_improvements",
                    "success": False,
                    "sql_query": improved_sql,
                    "error": query_result.get('error', ''),
                    "processing_time": processing_time,
                    "improvements_applied": True,
                    "improvement_failed": True
                }
                
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"Improved SQL execution failed: {str(e)}")
            return {
                "execution_type": "execute_with_improvements",
                "success": False,
                "sql_query": improved_sql,
                "error": str(e),
                "processing_time": processing_time
            }
    
    def _update_generation_stats(self, processing_time: float):
        """ÏÉùÏÑ± ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏"""
        total_queries = self.performance_stats["simple_queries"] + self.performance_stats["complex_queries"]
        if total_queries > 0:
            current_avg = self.performance_stats["avg_generation_time"]
            new_avg = ((current_avg * (total_queries - 1)) + processing_time) / total_queries
            self.performance_stats["avg_generation_time"] = new_avg
    
    def _add_to_generation_history(self, query: str, result: Dict):
        """ÏÉùÏÑ± ÌûàÏä§ÌÜ†Î¶¨Ïóê Ï∂îÍ∞Ä"""
        self.generation_history.append({
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "generation_type": result.get("generation_type"),
            "complexity": result.get("complexity"),
            "processing_time": result.get("processing_time", 0),
            "optimization_applied": result.get("optimization_applied", False)
        })
        
        # ÌûàÏä§ÌÜ†Î¶¨ ÌÅ¨Í∏∞ Ï†úÌïú
        if len(self.generation_history) > 50:
            self.generation_history = self.generation_history[-50:]
    
    def _create_fallback_result(self, generation_type: str, error_msg: str) -> Dict[str, Any]:
        """ÏÉùÏÑ± Ïã§Ìå®Ïãú ÎåÄÏ≤¥ Í≤∞Í≥º ÏÉùÏÑ±"""
        return {
            "generation_type": generation_type,
            "sql_query": "-- SQL ÏÉùÏÑ± Ïã§Ìå®Î°ú Ïù∏Ìïú Í∏∞Î≥∏ ÏøºÎ¶¨\nSELECT 'ERROR' as message;",
            "error": error_msg,
            "fallback": True,
            "confidence": 0.0
        }
    
    def get_agent_statistics(self) -> Dict[str, Any]:
        """Agent ÌÜµÍ≥Ñ Ï†ïÎ≥¥ Î∞òÌôò"""
        total_queries = self.performance_stats["simple_queries"] + self.performance_stats["complex_queries"]
        
        if total_queries == 0:
            return {"message": "ÏÉùÏÑ± Ïù¥Î†•Ïù¥ ÏóÜÏäµÎãàÎã§."}
        
        optimization_rate = (self.performance_stats["optimization_applied"] / total_queries) * 100
        
        return {
            "total_generated": total_queries,
            "simple_queries": self.performance_stats["simple_queries"],
            "complex_queries": self.performance_stats["complex_queries"],
            "optimization_rate": round(optimization_rate, 2),
            "avg_generation_time": round(self.performance_stats["avg_generation_time"], 3),
            "performance_grade": "A" if optimization_rate > 70 and self.performance_stats["avg_generation_time"] < 2.0 else "B"
        }

    def _print_query_results(self, query_result: Dict[str, Any]):
        """ÏøºÎ¶¨ Ïã§Ìñâ Í≤∞Í≥ºÎ•º Î≥¥Í∏∞ Ï¢ãÍ≤å Ï∂úÎ†•"""
        try:
            # Í∏∞Î≥∏ Ï†ïÎ≥¥ Ï∂úÎ†•
            print(f"\nüìã ÏøºÎ¶¨ Ïã§Ìñâ Ï†ïÎ≥¥:")
            print(f"   - Î∞òÌôòÎêú Ìñâ Ïàò: {query_result.get('returned_rows', 0)}Í∞ú")
            print(f"   - Ï≤òÎ¶¨Îêú Î∞îÏù¥Ìä∏: {query_result.get('total_bytes_processed', 0):,} bytes")
            print(f"   - Ïã§Ìñâ ÏãúÍ∞Ñ: {query_result.get('execution_time', 0):.2f}Ï¥à")
            
            # Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Ï∂úÎ†•
            if 'data' in query_result and query_result['data']:
                data = query_result['data']
                print(f"\nüìä ÏøºÎ¶¨ Í≤∞Í≥º Îç∞Ïù¥ÌÑ∞:")
                print("=" * 80)
                
                # Ïª¨ÎüºÎ™Ö Ï∂úÎ†•
                if data and len(data) > 0:
                    columns = list(data[0].keys())
                    header = " | ".join(f"{col:<20}" for col in columns)
                    print(f"   {header}")
                    print("   " + "-" * len(header))
                    
                    # Îç∞Ïù¥ÌÑ∞ Ìñâ Ï∂úÎ†• (ÏµúÎåÄ 10Í∞ú)
                    max_rows = min(10, len(data))
                    for i, row in enumerate(data[:max_rows]):
                        row_str = " | ".join(f"{str(val):<20}" for val in row.values())
                        print(f"   {row_str}")
                    
                    if len(data) > max_rows:
                        print(f"   ... (Ï¥ù {len(data)}Í∞ú Ìñâ Ï§ë {max_rows}Í∞úÎßå ÌëúÏãú)")
                
                print("=" * 80)
            
            # Ïò§Î•òÍ∞Ä ÏûàÎäî Í≤ΩÏö∞
            if 'error' in query_result and query_result['error']:
                print(f"\n‚ö†Ô∏è  Í≤ΩÍ≥†: {query_result['error']}")
                
        except Exception as e:
            print(f"‚ùå Í≤∞Í≥º Ï∂úÎ†• Ï§ë Ïò§Î•ò: {str(e)}")
            print(f"üìä ÏõêÎ≥∏ Í≤∞Í≥º: {query_result}")

# Agent ÏÉùÏÑ± Ìó¨Ìçº Ìï®Ïàò
def create_sql_generator_agent(custom_config: Optional[Dict[str, Any]] = None) -> SQLGeneratorAgent:
    """SQLGenerator Agent ÏÉùÏÑ±"""
    config = create_agent_config(
        name="sql_generator",
        specialization="sql_design_optimization",
        model="gpt-4",
        temperature=0.1,
        max_tokens=1500,
        **(custom_config or {})
    )
    
    return SQLGeneratorAgent(config)